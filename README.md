### **プロジェクト概要**

本プロジェクトは、AWSのサーバーレスサービスを活用したデータパイプライン構築の実践です。従来のサーバーを立てて行うETL処理とは異なり、ファイルアップロードという**イベントをトリガーにデータ処理を自動化**する仕組みを構築しました。

***

### **目的**

* **技術習得**: サーバーレスアーキテクチャの理解と、AWS Glue、Lambda、S3といった主要なAWSサービス間の連携を実践的に習得する。
* **AWS ETLサービスの理解**: ETL（Extract, Transform, Load）工程で頻繁に利用されるAWSサービスの役割と連携方法を深く理解する。
* **データ処理の自動化**: 大規模なデータや非定形なデータの整形・加工を効率的に行うための、イベント駆動型のパイプラインを構築する。

***

### **技術スタック**

* **クラウドプラットフォーム**: AWS
* **主要サービス**:
    * **AWS Glue**: ETL処理（データ変換・加工）
    * **AWS Lambda**: イベント駆動型トリガー
    * **Amazon S3**: データレイク（生データ、加工済みデータ、クエリ結果の保存）
    * **Amazon Athena**: SQLによる分析
    * **AWS IAM**: サービス間の権限管理
* **プログラミング言語**: Python (Lambda関数、GlueジョブのPySparkスクリプト)

***

### **アーキテクチャー**

本プロジェクトのアーキテクチャーは、以下のステップで構成される**イベント駆動型パイプライン**です。 

1.  外部から提供された生データ（CSV）が**S3バケット**にアップロードされます。
2.  S3にファイルが作成されると、**S3イベント通知**が**AWS Lambda**を自動的に起動します。
3.  Lambda関数は、S3イベントから取得した情報をもとに、**AWS Glueジョブ**を起動します。
4.  Glueジョブは、S3から生データを読み込み、変換処理を実行した後に、**Parquet形式**でS3の別のバケットに保存します。
5.  **Glueクローラー**が加工済みデータをスキャンし、**Glueデータカタログ**にテーブルとして登録します。
6.  **Amazon Athena**は、このデータカタログを参照し、SQLを使用してS3上のParquetファイルに直接クエリを実行します。

***

### **主要ライブラリ**

* **boto3**: PythonからAWSサービス（特にLambdaからGlueジョブを起動）を操作するための公式SDK。
* **PySpark**: AWS GlueのETLジョブでデータを分散処理するための主要なフレームワーク。

***

### **今後の展望**

* **より大規模なデータへの対応**: データの前処理段階でスキーマの事前定義（`AWS Glue Schema Registry`など）を行うことで、データの品質管理を強化し、大規模なデータセットでも安定して処理できるようにする。
* **パイプラインの効率化**: 処理段階ごとのフォルダ管理をより整理し、パーティショニングを最適化することで、クエリコストと実行時間を削減する。
* **スクリプトの改善**: ジョブの再利用性を高めるためのスクリプトのモジュール化や、エラーハンドリングの強化など、コードの品質向上を図る。